{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (run once per environment)\n",
        "%pip install -q dspy python-dotenv requests beautifulsoup4 lxml pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:04:06,100 INFO DSPy configured. Competitor monitoring agent ready.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DSPy configured. Competitor monitoring agent ready.\n"
          ]
        }
      ],
      "source": [
        "# Basic imports and environment setup\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import dspy\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import io\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "\n",
        "# Load .env for API keys and SMTP configuration\n",
        "load_dotenv()\n",
        "\n",
        "# Configure DSPy LM (align with other notebooks)\n",
        "lm = dspy.LM(\n",
        "    \"openai/gpt-5-mini\",\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=1,\n",
        "    max_tokens=16000,\n",
        ")\n",
        "\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "# Logging and timeouts\n",
        "LOG_LEVEL = os.getenv(\"COMP_MONITOR_LOG_LEVEL\", \"INFO\").upper()\n",
        "logging.basicConfig(\n",
        "    level=getattr(logging, LOG_LEVEL, logging.INFO),\n",
        "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(\"competitor_monitor\")\n",
        "\n",
        "HTTP_TIMEOUT_SEC = int(os.getenv(\"COMP_MONITOR_HTTP_TIMEOUT\", \"20\"))\n",
        "LM_TIMEOUT_SEC = int(os.getenv(\"COMP_MONITOR_LM_TIMEOUT\", \"60\"))\n",
        "MAX_SPR_PAGES_DEFAULT = int(os.getenv(\"COMP_MONITOR_MAX_SPR_PAGES\", \"25\"))\n",
        "MAX_LINKS_PER_SITE = int(os.getenv(\"COMP_MONITOR_MAX_LINKS_PER_SITE\", \"5\"))\n",
        "\n",
        "# HTTP defaults\n",
        "DEFAULT_HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "}\n",
        "\n",
        "# Simple in-repo persistence for URL diffing\n",
        "STATE_DIR = Path(\"state\")\n",
        "STATE_DIR.mkdir(exist_ok=True)\n",
        "URL_STORE_PATH = STATE_DIR / \"competitor_urls.json\"\n",
        "\n",
        "# Global working state\n",
        "monitor_state = {\n",
        "    \"competitors\": [],            # rows parsed from CSV\n",
        "    \"discovered_urls\": [],        # all discovered links (normalized, in-domain)\n",
        "    \"new_urls\": [],               # links not seen before, persisted with date_found\n",
        "    \"spr_items\": [],              # [{source_url,title,date,spr:[...]}, ...]\n",
        "    \"item_summaries\": [],         # [str]\n",
        "    \"executive_summary\": \"\",     # str\n",
        "    \"email_html\": \"\",            # str\n",
        "}\n",
        "\n",
        "logger.info(\"DSPy configured. Competitor monitoring agent ready.\")\n",
        "print(\"DSPy configured. Competitor monitoring agent ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:06:18,989 INFO Helpers ready: CSV, HTTP, HTML, normalization, persistence.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpers ready: CSV, HTTP, HTML, normalization, persistence.\n"
          ]
        }
      ],
      "source": [
        "# Helper functions: CSV fetch, HTML parsing, URL normalization, and persistence\n",
        "\n",
        "def run_with_timeout(func, timeout_sec: int, label: str):\n",
        "    start = time.perf_counter()\n",
        "    executor = ThreadPoolExecutor(max_workers=1)\n",
        "    future = executor.submit(func)\n",
        "    try:\n",
        "        result = future.result(timeout=timeout_sec)\n",
        "        elapsed = time.perf_counter() - start\n",
        "        logger.info(f\"{label} completed in {elapsed:.2f}s\")\n",
        "        return result\n",
        "    except FuturesTimeoutError:\n",
        "        elapsed = time.perf_counter() - start\n",
        "        logger.warning(f\"{label} timed out after {elapsed:.2f}s (limit {timeout_sec}s)\")\n",
        "        # Best-effort cancel; underlying operation may continue in background\n",
        "        future.cancel()\n",
        "        raise\n",
        "    finally:\n",
        "        # Return control immediately; don't wait for unfinished tasks\n",
        "        try:\n",
        "            executor.shutdown(wait=False, cancel_futures=True)\n",
        "        except TypeError:\n",
        "            executor.shutdown(wait=False)\n",
        "\n",
        "\n",
        "def read_competitor_csv(csv_url: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Download a published CSV of competitors. Expected columns:\n",
        "    - 'Url': canonical domain (e.g., https://example.com)\n",
        "    - 'blog urls': blog or news root (e.g., https://example.com/blog)\n",
        "    Returns a list of row dicts.\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    logger.info(f\"Fetching competitors CSV: {csv_url}\")\n",
        "    resp = requests.get(csv_url, timeout=HTTP_TIMEOUT_SEC)\n",
        "    resp.raise_for_status()\n",
        "    df = pd.read_csv(io.StringIO(resp.text))\n",
        "    # Keep only expected columns if present\n",
        "    rows = []\n",
        "    for _, r in df.iterrows():\n",
        "        row = {k: (str(r[k]).strip() if k in r and pd.notna(r[k]) else \"\") for k in [\"Url\", \"blog urls\"]}\n",
        "        if row[\"Url\"] and row[\"blog urls\"]:\n",
        "            rows.append(row)\n",
        "    logger.info(f\"CSV parsed: {len(rows)} rows in {time.perf_counter()-t0:.2f}s\")\n",
        "    return rows\n",
        "\n",
        "\n",
        "def fetch_html(url: str, headers: dict | None = None, max_redirects: int = 2) -> str:\n",
        "    \"\"\"\n",
        "    Minimal HTTP GET for HTML content. Returns response text or raises.\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    session = requests.Session()\n",
        "    session.max_redirects = max_redirects\n",
        "    logger.info(f\"HTTP GET {url}\")\n",
        "    resp = session.get(url, headers=headers or DEFAULT_HEADERS, timeout=HTTP_TIMEOUT_SEC)\n",
        "    resp.raise_for_status()\n",
        "    resp.encoding = resp.encoding or 'utf-8'\n",
        "    html = resp.text\n",
        "    logger.info(f\"HTTP {url} -> {len(html)} bytes in {time.perf_counter()-t0:.2f}s\")\n",
        "    return html\n",
        "\n",
        "\n",
        "def extract_links(html: str, base_url: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Extract absolute links from HTML, normalized against base_url.\n",
        "    Preserves document order and strips URL fragments.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    links_ordered: list[str] = []\n",
        "    seen: set[str] = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"].strip()\n",
        "        if not href:\n",
        "            continue\n",
        "        abs_url = urljoin(base_url, href)\n",
        "        parsed = urlparse(abs_url)\n",
        "        if parsed.scheme in {\"http\", \"https\"}:\n",
        "            cleaned = parsed._replace(fragment=\"\").geturl()\n",
        "            if cleaned not in seen:\n",
        "                seen.add(cleaned)\n",
        "                links_ordered.append(cleaned)\n",
        "    logger.info(f\"Extracted {len(links_ordered)} links from {base_url}\")\n",
        "    return links_ordered\n",
        "\n",
        "\n",
        "def same_host(url_a: str, url_b: str) -> bool:\n",
        "    \"\"\"\n",
        "    True if url_a and url_b share the same netloc (host:port), ignoring scheme.\n",
        "    \"\"\"\n",
        "    pa, pb = urlparse(url_a), urlparse(url_b)\n",
        "    return pa.netloc.lower() == pb.netloc.lower()\n",
        "\n",
        "\n",
        "def filter_articleish_links(links: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Keep links that look like articles/posts and drop assets, auth, feeds, and anchors.\n",
        "    Heuristics are simple and conservative.\n",
        "    \"\"\"\n",
        "    drop_substrings = [\n",
        "        \"#\", \"?share=\", \"?utm_\", \"/tag/\", \"/category/\", \"/topics/\", \"/author/\",\n",
        "        \"/login\", \"/signin\", \"/signup\", \"/register\", \"/account\", \"/privacy\", \"/terms\",\n",
        "        \"/feed\", \"/rss\", \"/atom\", \".xml\",\n",
        "    ]\n",
        "    drop_extensions = {\n",
        "        \".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\", \".svg\", \".ico\",\n",
        "        \".css\", \".js\", \".pdf\", \".zip\", \".mp4\", \".mov\", \".avi\", \".woff\", \".woff2\", \".ttf\",\n",
        "    }\n",
        "    kept: list[str] = []\n",
        "    for url in links:\n",
        "        lower = url.lower()\n",
        "        if any(s in lower for s in drop_substrings):\n",
        "            continue\n",
        "        path = urlparse(lower).path\n",
        "        if any(path.endswith(ext) for ext in drop_extensions):\n",
        "            continue\n",
        "        kept.append(url)\n",
        "    return kept\n",
        "\n",
        "\n",
        "def load_url_store() -> dict:\n",
        "    \"\"\"\n",
        "    Load persisted URL store from disk. Format:\n",
        "    {\n",
        "      \"urls\": {\n",
        "        \"https://example.com/post\": {\"source_site\": \"https://example.com\", \"date_found\": \"ISO8601\"}\n",
        "      }\n",
        "    }\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    if URL_STORE_PATH.exists():\n",
        "        with URL_STORE_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        logger.info(f\"Loaded URL store with {len(data.get('urls', {}))} entries in {time.perf_counter()-t0:.2f}s\")\n",
        "        return data\n",
        "    logger.info(\"URL store not found; starting fresh\")\n",
        "    return {\"urls\": {}}\n",
        "\n",
        "\n",
        "def save_url_store(store: dict) -> None:\n",
        "    t0 = time.perf_counter()\n",
        "    URL_STORE_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with URL_STORE_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(store, f, indent=2)\n",
        "    logger.info(f\"Saved URL store with {len(store.get('urls', {}))} entries in {time.perf_counter()-t0:.2f}s\")\n",
        "\n",
        "\n",
        "def upsert_new_urls(discovered: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Persist newly discovered URLs, returning only new ones.\n",
        "    Each discovered item: {\"normalized_url\": str, \"source_site\": str}\n",
        "    \"\"\"\n",
        "    t0 = time.perf_counter()\n",
        "    store = load_url_store()\n",
        "    known = store.get(\"urls\", {})\n",
        "    new_items: list[dict] = []\n",
        "    for item in discovered:\n",
        "        url = item[\"normalized_url\"]\n",
        "        if url not in known:\n",
        "            known[url] = {\n",
        "                \"source_site\": item.get(\"source_site\", \"\"),\n",
        "                \"date_found\": datetime.now(timezone.utc).isoformat(),\n",
        "            }\n",
        "            new_items.append({\n",
        "                \"normalized_url\": url,\n",
        "                \"source_site\": item.get(\"source_site\", \"\"),\n",
        "                \"date_found\": known[url][\"date_found\"],\n",
        "            })\n",
        "    store[\"urls\"] = known\n",
        "    save_url_store(store)\n",
        "    logger.info(f\"Discovered {len(discovered)} urls; {len(new_items)} new in {time.perf_counter()-t0:.2f}s\")\n",
        "    return new_items\n",
        "\n",
        "logger.info(\"Helpers ready: CSV, HTTP, HTML, normalization, persistence.\")\n",
        "print(\"Helpers ready: CSV, HTTP, HTML, normalization, persistence.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Signatures and predictors ready.\n"
          ]
        }
      ],
      "source": [
        "# DSPy signatures and predictors\n",
        "\n",
        "class SPRSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Render page content as a Sparse Priming Representation (SPR) for downstream LLM use.\n",
        "    Output JSON array with: source_url, title, date, spr (list of statements).\n",
        "    \"\"\"\n",
        "    source_url: str = dspy.InputField()\n",
        "    page_text: str = dspy.InputField()\n",
        "    title_hint: str = dspy.InputField()\n",
        "    spr_json: str = dspy.OutputField(description=\"JSON array string as specified\")\n",
        "\n",
        "\n",
        "class ItemSummarySignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Summarize a single competitor page based on its SPR into 2–3 sentences.\n",
        "    Audience: business owner. Reference the source and label competitor.\n",
        "    \"\"\"\n",
        "    competitor_name: str = dspy.InputField()\n",
        "    source_url: str = dspy.InputField()\n",
        "    spr_json: str = dspy.InputField()\n",
        "    summary: str = dspy.OutputField()\n",
        "\n",
        "\n",
        "class ExecSummarySignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Aggregate multiple item summaries and write a concise 2–3 sentence\n",
        "    executive summary highlighting trends, themes, and outliers. Output JSON only.\n",
        "    \"\"\"\n",
        "    summaries_json: str = dspy.InputField()\n",
        "    executive_summary_json: str = dspy.OutputField()\n",
        "\n",
        "\n",
        "spr_predict = dspy.Predict(SPRSignature)\n",
        "item_summary_predict = dspy.Predict(ItemSummarySignature)\n",
        "exec_summary_predict = dspy.Predict(ExecSummarySignature)\n",
        "\n",
        "print(\"Signatures and predictors ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:10:05,257 INFO Tools ready: get_competitors, discover_new_urls, generate_spr, summarize_items, exec_summary, build_email_html, send_email_smtp\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tools ready: get_competitors, discover_new_urls, generate_spr, summarize_items, exec_summary, build_email_html, send_email_smtp\n"
          ]
        }
      ],
      "source": [
        "# Tools: mirror n8n nodes as callable functions returning dicts for agent state mgmt\n",
        "\n",
        "# 1) Get competitors CSV\n",
        "\n",
        "def tool_get_competitors(csv_url: str) -> dict:\n",
        "    t0 = time.perf_counter()\n",
        "    logger.info(f\"tool_get_competitors: start csv_url={csv_url}\")\n",
        "    rows = read_competitor_csv(csv_url)\n",
        "    monitor_state[\"competitors\"] = rows\n",
        "    logger.info(f\"tool_get_competitors: loaded {len(rows)} competitors in {time.perf_counter()-t0:.2f}s\")\n",
        "    return {\"tool\": \"get_competitors\", \"count\": len(rows)}\n",
        "\n",
        "\n",
        "# 2) Discover new URLs from each competitor blog root\n",
        "\n",
        "def tool_discover_new_urls() -> dict:\n",
        "    t0 = time.perf_counter()\n",
        "    discovered: list[dict] = []\n",
        "    competitors = monitor_state.get(\"competitors\", [])\n",
        "    logger.info(f\"tool_discover_new_urls: start for {len(competitors)} competitors\")\n",
        "    for row in competitors:\n",
        "        site = row[\"Url\"].strip()\n",
        "        blog_root = row[\"blog urls\"].strip()\n",
        "        try:\n",
        "            logger.info(f\"Discovering links from {blog_root} (site {site})\")\n",
        "            html = fetch_html(blog_root)\n",
        "            links = extract_links(html, blog_root)\n",
        "            # Filter to same-host and article-ish\n",
        "            same_host_links = [u for u in links if same_host(u, site) or same_host(u, blog_root)]\n",
        "            filtered = filter_articleish_links(same_host_links)\n",
        "            # Heuristic: most recent links tend to appear later or earlier depending on theme.\n",
        "            # To be robust, take the last N unique in document order (preserved) which often correspond to recent posts widgets.\n",
        "            limited = filtered[-MAX_LINKS_PER_SITE:] if MAX_LINKS_PER_SITE > 0 else filtered\n",
        "            logger.info(\n",
        "                f\"{blog_root}: extracted={len(links)} same_host={len(same_host_links)} articleish={len(filtered)} kept={len(limited)}\"\n",
        "            )\n",
        "            for link in limited:\n",
        "                discovered.append({\n",
        "                    \"normalized_url\": link,\n",
        "                    \"source_site\": site,\n",
        "                })\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Discovery failed for {blog_root}: {e}\")\n",
        "            continue\n",
        "    # de-duplicate by URL while preserving last occurrence (recent-first bias)\n",
        "    unique: dict[str, dict] = {}\n",
        "    for item in discovered:\n",
        "        unique[item[\"normalized_url\"]] = item\n",
        "    discovered_list = list(unique.values())\n",
        "    monitor_state[\"discovered_urls\"] = discovered_list\n",
        "    logger.info(\n",
        "        f\"tool_discover_new_urls: found {len(discovered)} links, {len(discovered_list)} unique in {time.perf_counter()-t0:.2f}s\"\n",
        "    )\n",
        "\n",
        "    # Persist and compute diff\n",
        "    new_items = upsert_new_urls(discovered_list)\n",
        "    monitor_state[\"new_urls\"] = new_items\n",
        "    logger.info(f\"tool_discover_new_urls: {len(new_items)} new URLs\")\n",
        "    return {\"tool\": \"discover_new_urls\", \"new_count\": len(new_items)}\n",
        "\n",
        "\n",
        "# 3) Generate SPR for each new URL\n",
        "\n",
        "def tool_generate_spr(max_pages: int = MAX_SPR_PAGES_DEFAULT) -> dict:\n",
        "    t0 = time.perf_counter()\n",
        "    spr_items: list[dict] = []\n",
        "    urls = monitor_state.get(\"new_urls\", [])[:max_pages]\n",
        "    logger.info(f\"tool_generate_spr: start for {len(urls)} pages (cap={max_pages})\")\n",
        "    for item in urls:\n",
        "        url = item[\"normalized_url\"]\n",
        "        try:\n",
        "            html = fetch_html(url)\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "            title = soup.title.string.strip() if soup.title and soup.title.string else \"unknown\"\n",
        "            # Simple text extraction\n",
        "            for s in soup([\"script\", \"style\", \"noscript\"]):\n",
        "                s.extract()\n",
        "            text = \" \".join(soup.get_text(separator=\" \").split())[:20000]\n",
        "\n",
        "            logger.info(f\"SPR predict start: {url} (text_len={len(text)})\")\n",
        "            pred = run_with_timeout(\n",
        "                lambda: spr_predict(source_url=url, page_text=text, title_hint=title),\n",
        "                LM_TIMEOUT_SEC,\n",
        "                f\"SPR predict {url}\",\n",
        "            )\n",
        "            spr_items.append({\n",
        "                \"source_url\": url,\n",
        "                \"title\": title,\n",
        "                \"spr_json\": pred.spr_json,\n",
        "                \"date_found\": item.get(\"date_found\", \"unknown\"),\n",
        "            })\n",
        "        except FuturesTimeoutError:\n",
        "            logger.warning(f\"SPR timeout for {url}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"SPR failed for {url}: {e}\")\n",
        "            continue\n",
        "    monitor_state[\"spr_items\"] = spr_items\n",
        "    logger.info(f\"tool_generate_spr: completed {len(spr_items)} items in {time.perf_counter()-t0:.2f}s\")\n",
        "    return {\"tool\": \"generate_spr\", \"count\": len(spr_items)}\n",
        "\n",
        "\n",
        "# 4) Summarize items (per-URL summaries)\n",
        "\n",
        "def tool_summarize_items() -> dict:\n",
        "    t0 = time.perf_counter()\n",
        "    summaries: list[str] = []\n",
        "    for item in monitor_state.get(\"spr_items\", []):\n",
        "        source = item[\"source_url\"]\n",
        "        comp = urlparse(item.get(\"source_url\", \"\")).netloc\n",
        "        try:\n",
        "            logger.info(f\"Item summary start: {source}\")\n",
        "            pred = run_with_timeout(\n",
        "                lambda: item_summary_predict(\n",
        "                    competitor_name=comp,\n",
        "                    source_url=source,\n",
        "                    spr_json=item[\"spr_json\"],\n",
        "                ),\n",
        "                LM_TIMEOUT_SEC,\n",
        "                f\"Item summary {source}\",\n",
        "            )\n",
        "            summaries.append(pred.summary)\n",
        "        except FuturesTimeoutError:\n",
        "            logger.warning(f\"Item summary timeout: {source}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Item summary failed for {source}: {e}\")\n",
        "            continue\n",
        "    monitor_state[\"item_summaries\"] = summaries\n",
        "    logger.info(f\"tool_summarize_items: {len(summaries)} summaries in {time.perf_counter()-t0:.2f}s\")\n",
        "    return {\"tool\": \"summarize_items\", \"count\": len(summaries)}\n",
        "\n",
        "\n",
        "# 5) Executive summary\n",
        "\n",
        "def tool_exec_summary() -> dict:\n",
        "    t0 = time.perf_counter()\n",
        "    # wrap summaries as JSON array for the signature\n",
        "    payload = json.dumps(monitor_state.get(\"item_summaries\", []))\n",
        "    try:\n",
        "        pred = run_with_timeout(\n",
        "            lambda: exec_summary_predict(summaries_json=payload),\n",
        "            LM_TIMEOUT_SEC,\n",
        "            \"Exec summary\",\n",
        "        )\n",
        "        # Expect JSON string in output\n",
        "        monitor_state[\"executive_summary\"] = pred.executive_summary_json\n",
        "    except FuturesTimeoutError:\n",
        "        logger.warning(\"Executive summary timeout; leaving as empty string\")\n",
        "        monitor_state[\"executive_summary\"] = \"{}\"\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Executive summary failed: {e}\")\n",
        "        monitor_state[\"executive_summary\"] = \"{}\"\n",
        "    logger.info(f\"tool_exec_summary: done in {time.perf_counter()-t0:.2f}s\")\n",
        "    return {\"tool\": \"exec_summary\"}\n",
        "\n",
        "\n",
        "# 6) Build email HTML (no sending here)\n",
        "\n",
        "def tool_build_email_html(to_email: str, subject: str = \"daily competitor digest\") -> dict:\n",
        "    logger.info(\"tool_build_email_html: start\")\n",
        "    exec_summary_json = monitor_state.get(\"executive_summary\", \"{}\")\n",
        "    try:\n",
        "        exec_text = json.loads(exec_summary_json)\n",
        "        if isinstance(exec_text, dict) and \"summary\" in exec_text:\n",
        "            exec_html = exec_text[\"summary\"]\n",
        "        elif isinstance(exec_text, str):\n",
        "            exec_html = exec_text\n",
        "        else:\n",
        "            exec_html = json.dumps(exec_text)\n",
        "    except Exception:\n",
        "        exec_html = exec_summary_json\n",
        "\n",
        "    items = monitor_state.get(\"item_summaries\", [])\n",
        "    items_html = \"\".join([f\"<li>{s}</li>\" for s in items])\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <p>Hi,</p>\n",
        "    <p>Here’s the daily scoop:</p>\n",
        "    <h3>Executive Summary</h3>\n",
        "    <p>{exec_html}</p>\n",
        "    <h3>Competitor Updates</h3>\n",
        "    <ul>\n",
        "    {items_html}\n",
        "    </ul>\n",
        "    <p>Have a great day,</p>\n",
        "    <hr>\n",
        "    <p><em>This email was sent automatically by the DSPy agent</em></p>\n",
        "    \"\"\".strip()\n",
        "\n",
        "    monitor_state[\"email_html\"] = html\n",
        "    logger.info(f\"tool_build_email_html: built html length={len(html)} items={len(items)}\")\n",
        "    return {\"tool\": \"build_email_html\", \"to\": to_email, \"subject\": subject}\n",
        "\n",
        "\n",
        "# 7) Send email via SMTP (optional; stub to avoid side-effects)\n",
        "\n",
        "def tool_send_email_smtp(to_email: str, subject: str) -> dict:\n",
        "    \"\"\"\n",
        "    Minimal SMTP stub to avoid side effects. Implement with real SMTP creds if needed.\n",
        "    \"\"\"\n",
        "    logger.info(f\"tool_send_email_smtp: stub send to={to_email} subject={subject}\")\n",
        "    # In this repo we avoid actually sending email; return payload for inspection\n",
        "    return {\n",
        "        \"tool\": \"send_email_smtp\",\n",
        "        \"to\": to_email,\n",
        "        \"subject\": subject,\n",
        "        \"html_length\": len(monitor_state.get(\"email_html\", \"\")),\n",
        "    }\n",
        "\n",
        "logger.info(\"Tools ready: get_competitors, discover_new_urls, generate_spr, summarize_items, exec_summary, build_email_html, send_email_smtp\")\n",
        "print(\"Tools ready: get_competitors, discover_new_urls, generate_spr, summarize_items, exec_summary, build_email_html, send_email_smtp\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:10:26,846 INFO ReAct agent ready.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ReAct agent ready.\n"
          ]
        }
      ],
      "source": [
        "# ReAct agent definition\n",
        "\n",
        "class CompetitorAgentSignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Competitor Monitoring Agent\n",
        "    Tasks:\n",
        "    - Fetch competitors CSV\n",
        "    - Crawl blog roots, discover in-domain links, diff against store\n",
        "    - For new URLs: generate SPR, write per-item summaries\n",
        "    - Aggregate an executive summary\n",
        "    - Build email HTML (and optionally send)\n",
        "    Return final HTML in `email_html_out`.\n",
        "    \"\"\"\n",
        "    csv_url: str = dspy.InputField()\n",
        "    to_email: str = dspy.InputField()\n",
        "    reasoning: str = dspy.OutputField()\n",
        "    email_html_out: str = dspy.OutputField()\n",
        "\n",
        "\n",
        "competitor_agent = dspy.ReAct(\n",
        "    CompetitorAgentSignature,\n",
        "    tools=[\n",
        "        tool_get_competitors,\n",
        "        tool_discover_new_urls,\n",
        "        tool_generate_spr,\n",
        "        tool_summarize_items,\n",
        "        tool_exec_summary,\n",
        "        tool_build_email_html,\n",
        "        tool_send_email_smtp,\n",
        "    ],\n",
        ")\n",
        "\n",
        "logger.info(\"ReAct agent ready.\")\n",
        "print(\"ReAct agent ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-10 18:10:31,174 INFO Demo start: csv=https://docs.google.com/spreadsheets/d/e/2PACX-1vTvE_qRDWRu5hjZ45yY6juAc4i7iT3DIrJT9q3cz29uIpGpz0IRRzHPdqcKge8obrTjwNS7qC3TGGg-/pub?gid=0&single=true&output=csv to=youremailhere\n",
            "2025-10-10 18:10:36,735 INFO tool_get_competitors: start csv_url=https://docs.google.com/spreadsheets/d/e/2PACX-1vTvE_qRDWRu5hjZ45yY6juAc4i7iT3DIrJT9q3cz29uIpGpz0IRRzHPdqcKge8obrTjwNS7qC3TGGg-/pub?gid=0&single=true&output=csv\n",
            "2025-10-10 18:10:36,736 INFO Fetching competitors CSV: https://docs.google.com/spreadsheets/d/e/2PACX-1vTvE_qRDWRu5hjZ45yY6juAc4i7iT3DIrJT9q3cz29uIpGpz0IRRzHPdqcKge8obrTjwNS7qC3TGGg-/pub?gid=0&single=true&output=csv\n",
            "2025-10-10 18:10:37,941 INFO CSV parsed: 1 rows in 1.21s\n",
            "2025-10-10 18:10:37,943 INFO tool_get_competitors: loaded 1 competitors in 1.21s\n",
            "2025-10-10 18:10:42,580 INFO tool_discover_new_urls: start for 1 competitors\n",
            "2025-10-10 18:10:42,580 INFO Discovering links from https://askrally.com/articles (site https://askrally.com)\n",
            "2025-10-10 18:10:42,581 INFO HTTP GET https://askrally.com/articles\n",
            "2025-10-10 18:10:43,877 INFO HTTP https://askrally.com/articles -> 11258 bytes in 1.30s\n",
            "2025-10-10 18:10:43,880 INFO Extracted 0 links from https://askrally.com/articles\n",
            "2025-10-10 18:10:43,880 INFO https://askrally.com/articles: extracted=0 same_host=0 articleish=0 kept=0\n",
            "2025-10-10 18:10:43,881 INFO tool_discover_new_urls: found 0 links, 0 unique in 1.30s\n",
            "2025-10-10 18:10:43,881 INFO Loaded URL store with 10 entries in 0.00s\n",
            "2025-10-10 18:10:43,882 INFO Saved URL store with 10 entries in 0.00s\n",
            "2025-10-10 18:10:43,883 INFO Discovered 0 urls; 0 new in 0.00s\n",
            "2025-10-10 18:10:43,883 INFO tool_discover_new_urls: 0 new URLs\n",
            "2025-10-10 18:11:01,636 INFO Exec summary completed in 11.81s\n",
            "2025-10-10 18:11:01,637 INFO tool_exec_summary: done in 11.81s\n",
            "2025-10-10 18:11:11,371 INFO tool_build_email_html: start\n",
            "2025-10-10 18:11:11,372 INFO tool_build_email_html: built html length=484 items=0\n",
            "2025-10-10 18:11:36,496 INFO Agent run completed in 65.32s\n",
            "2025-10-10 18:11:36,497 INFO tool_build_email_html: start\n",
            "2025-10-10 18:11:36,498 INFO tool_build_email_html: built html length=484 items=0\n",
            "2025-10-10 18:11:36,498 INFO Run summary: new_urls=0 items=0 html_len=484\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reasoning:\n",
            " I fetched the competitors CSV (1 competitor) and crawled the listed blog root(s). I discovered in-domain links and compared them to the existing store of known URLs. No new URLs were found (new_count = 0), so no SPRs or per-item summaries were generated.\n",
            "\n",
            "Summary of actions:\n",
            "- Fetched competitors CSV (source provided).\n",
            "- Crawled blog root(s) for 1 competitor.\n",
            "- Discovered in-domain links and diffed against the store.\n",
            "- New URLs found: 0 — no new SPRs or summaries created.\n",
            "- Executive summary prepared.\n",
            "\n",
            "Recommended next steps:\n",
            "- Continue daily monitoring; the pipeline is working and found no new content today.\n",
            "- If you want broader coverage, consider including sitemaps, RSS feeds, or deeper crawl depths.\n",
            "- If you want alerts for specific content types (product mentions, pricing, promos), I can add keyword filters and escalation rules.\n",
            "- If you want the HTML emailed automatically on each run, I can enable a send step.\n",
            "\n",
            "Date of run: 2025-10-10\n",
            "\n",
            "New URLs: 0\n",
            "Items summarized: 0\n",
            "\n",
            "Executive summary (JSON):\n",
            " {\"executive_summary_json\":\"No item summaries were provided, so no trends, themes, or outliers can be identified. Please supply summaries or data to generate a concise executive summary highlighting trends, themes, and outliers.\"}\n",
            "\n",
            "Email preview (first 800 chars):\n",
            " <p>Hi,</p>\n",
            "    <p>Here’s the daily scoop:</p>\n",
            "    <h3>Executive Summary</h3>\n",
            "    <p>{\"executive_summary_json\": \"No item summaries were provided, so no trends, themes, or outliers can be identified. Please supply summaries or data to generate a concise executive summary highlighting trends, themes, and outliers.\"}</p>\n",
            "    <h3>Competitor Updates</h3>\n",
            "    <ul>\n",
            "    \n",
            "    </ul>\n",
            "    <p>Have a great day,</p>\n",
            "    <hr>\n",
            "    <p><em>This email was sent automatically by the DSPy agent</em></p>\n"
          ]
        }
      ],
      "source": [
        "# Demo: run the agent\n",
        "CSV_URL = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vTvE_qRDWRu5hjZ45yY6juAc4i7iT3DIrJT9q3cz29uIpGpz0IRRzHPdqcKge8obrTjwNS7qC3TGGg-/pub?gid=0&single=true&output=csv\"\n",
        "TO_EMAIL = os.getenv(\"COMP_MONITOR_TO_EMAIL\", \"youremailhere\")\n",
        "\n",
        "# Clear working state for demo\n",
        "monitor_state[\"competitors\"] = []\n",
        "monitor_state[\"discovered_urls\"] = []\n",
        "monitor_state[\"new_urls\"] = []\n",
        "monitor_state[\"spr_items\"] = []\n",
        "monitor_state[\"item_summaries\"] = []\n",
        "monitor_state[\"executive_summary\"] = \"\"\n",
        "monitor_state[\"email_html\"] = \"\"\n",
        "\n",
        "logger.info(f\"Demo start: csv={CSV_URL} to={TO_EMAIL}\")\n",
        "t0 = time.perf_counter()\n",
        "try:\n",
        "    result = competitor_agent(csv_url=CSV_URL, to_email=TO_EMAIL)\n",
        "    logger.info(f\"Agent run completed in {time.perf_counter()-t0:.2f}s\")\n",
        "except Exception as e:\n",
        "    logger.exception(f\"Agent run failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Build email HTML via tool explicitly to ensure output string\n",
        "tool_build_email_html(to_email=TO_EMAIL, subject=\"daily competitor digest\")\n",
        "\n",
        "logger.info(\n",
        "    f\"Run summary: new_urls={len(monitor_state['new_urls'])} items={len(monitor_state['item_summaries'])} html_len={len(monitor_state['email_html'])}\"\n",
        ")\n",
        "\n",
        "print(\"Reasoning:\\n\", result.reasoning)\n",
        "print(\"\\nNew URLs:\", len(monitor_state[\"new_urls\"]))\n",
        "print(\"Items summarized:\", len(monitor_state[\"item_summaries\"]))\n",
        "print(\"\\nExecutive summary (JSON):\\n\", monitor_state[\"executive_summary\"]) \n",
        "print(\"\\nEmail preview (first 800 chars):\\n\", monitor_state[\"email_html\"][:800])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
