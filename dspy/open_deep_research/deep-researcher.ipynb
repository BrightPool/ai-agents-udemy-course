{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once per environment)\n",
        "%pip install -q dspy python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic imports and environment setup\n",
        "import os\n",
        "import dspy\n",
        "from dotenv import load_dotenv\n",
        "from dspy import History\n",
        "\n",
        "# Load API keys from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Configure model provider (OpenAI-only, per LangGraph agent)\n",
        "lm = dspy.LM(\"openai/gpt-5-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0.2, max_tokens=12000)\n",
        "\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "# Conversation memory\n",
        "conversation_history = History(messages=[])\n",
        "\n",
        "print(\"DSPy configured for Deep Research agent.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utils: history sanitization and helpers\n",
        "from dspy import History as _History\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def sanitize_history(history: _History) -> _History:\n",
        "    \"\"\"Return a new History with only valid, non-empty messages.\n",
        "    - Keeps only dicts with role in {user, assistant} and non-empty content\n",
        "    - Converts legacy entries with keys 'user_message'/'answer' into role/content\n",
        "    \"\"\"\n",
        "    new_messages = []\n",
        "    for m in getattr(history, \"messages\", []):\n",
        "        if isinstance(m, dict):\n",
        "            role = m.get(\"role\")\n",
        "            content = m.get(\"content\")\n",
        "            if role in (\"user\", \"assistant\") and isinstance(content, str) and content.strip():\n",
        "                new_messages.append({\"role\": role, \"content\": content})\n",
        "                continue\n",
        "            if isinstance(m.get(\"user_message\"), str) and m[\"user_message\"].strip():\n",
        "                new_messages.append({\"role\": \"user\", \"content\": m[\"user_message\"]})\n",
        "                continue\n",
        "            if isinstance(m.get(\"answer\"), str) and m[\"answer\"].strip():\n",
        "                new_messages.append({\"role\": \"assistant\", \"content\": m[\"answer\"]})\n",
        "                continue\n",
        "    return _History(messages=new_messages)\n",
        "\n",
        "\n",
        "def get_today_str() -> str:\n",
        "    now = datetime.now()\n",
        "    return f\"{now:%a} {now:%b} {now.day}, {now:%Y}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tools: ported from LangGraph (OpenAI-only research + think tool)\n",
        "from typing import List, Literal\n",
        "\n",
        "\n",
        "def openai_search(queries: List[str], max_results: int = 5, topic: Literal[\"general\", \"news\"] = \"general\") -> str:\n",
        "    \"\"\"Generate comprehensive research responses using the model's knowledge base.\n",
        "\n",
        "    Mirrors the LangGraph `openai_search` tool behavior at a high level.\n",
        "    We keep a simple sequential loop for clarity in DSPy.\n",
        "    \"\"\"\n",
        "    if not isinstance(queries, list) or not queries:\n",
        "        return \"No research results could be generated. Please provide queries.\"\n",
        "\n",
        "    results = []\n",
        "    for q in queries[:max_results]:\n",
        "        if topic == \"news\":\n",
        "            prompt = f\"\"\"Please provide a comprehensive research summary for the following current events query: \"{q}\"\n",
        "\n",
        "Focus on recent developments and provide:\n",
        "1. Key facts and developments\n",
        "2. Timeline of important events\n",
        "3. Current status and implications\n",
        "4. Sources and references you know of\n",
        "\n",
        "Be thorough and objective. Include dates where relevant.\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"Please provide comprehensive information for the following research query: \"{q}\"\n",
        "\n",
        "Structure your response to include:\n",
        "1. Key facts and background information\n",
        "2. Important details and context\n",
        "3. Current understanding and implications\n",
        "4. Any relevant examples or case studies\n",
        "\n",
        "Be thorough and provide detailed, accurate information based on your knowledge.\"\"\"\n",
        "        # Single-turn call\n",
        "        resp = dspy.Predict(\"answer\")(question=prompt)  # lightweight call\n",
        "        content = getattr(resp, \"answer\", \"\") if isinstance(resp, dspy.Prediction) else str(resp)\n",
        "        results.append(f\"--- RESEARCH RESULT: {q} ---\\n{content}\\n\")\n",
        "\n",
        "    if not results:\n",
        "        return \"No research results could be generated.\"\n",
        "    return (f\"OpenAI Research Results ({topic} focus):\\n\\n\" + \"\\n\\n\".join(results)).strip()\n",
        "\n",
        "\n",
        "def think_tool(reflection: str) -> str:\n",
        "    \"\"\"Strategic reflection tool for research planning.\n",
        "\n",
        "    Use after searches to analyze results and plan next steps.\n",
        "    \"\"\"\n",
        "    return f\"Reflection recorded: {reflection}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Signatures for each phase (mirroring LangGraph prompts)\n",
        "\n",
        "class ClarifySignature(dspy.Signature):\n",
        "    \"\"\"\n",
        "    Analyze user's messages and decide whether to ask a clarifying question.\n",
        "\n",
        "    Return JSON-like outputs:\n",
        "    - need_clarification: boolean\n",
        "    - question: clarifying question to ask\n",
        "    - verification: verification message when proceeding\n",
        "    \"\"\"\n",
        "    messages: dspy.History = dspy.InputField()\n",
        "    date: str = dspy.InputField()\n",
        "\n",
        "    need_clarification: bool = dspy.OutputField()\n",
        "    question: str = dspy.OutputField()\n",
        "    verification: str = dspy.OutputField()\n",
        "\n",
        "\n",
        "class BriefSignature(dspy.Signature):\n",
        "    \"\"\"Turn messages into a structured research brief.\"\"\"\n",
        "    messages: dspy.History = dspy.InputField()\n",
        "    date: str = dspy.InputField()\n",
        "\n",
        "    research_brief: str = dspy.OutputField()\n",
        "\n",
        "\n",
        "class GenerateQueriesSignature(dspy.Signature):\n",
        "    \"\"\"Generate diverse, concrete queries from the brief.\"\"\"\n",
        "    messages: dspy.History = dspy.InputField()\n",
        "    research_brief: str = dspy.InputField()\n",
        "    date: str = dspy.InputField()\n",
        "    num_queries: int = dspy.InputField()\n",
        "\n",
        "    queries_text: str = dspy.OutputField(description=\"One query per line, no numbering\")\n",
        "\n",
        "\n",
        "class FinalReportSignature(dspy.Signature):\n",
        "    \"\"\"Generate a concise, well-structured final report from findings.\"\"\"\n",
        "    research_brief: str = dspy.InputField()\n",
        "    messages: dspy.History = dspy.InputField()\n",
        "    findings: str = dspy.InputField()\n",
        "    date: str = dspy.InputField()\n",
        "\n",
        "    final_report: str = dspy.OutputField()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple phase predictors (useful to keep behavior explicit for interns)\n",
        "\n",
        "clarify_predict = dspy.Predict(ClarifySignature)\n",
        "brief_predict = dspy.Predict(BriefSignature)\n",
        "queries_predict = dspy.Predict(GenerateQueriesSignature)\n",
        "final_report_predict = dspy.Predict(FinalReportSignature)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Orchestrator: clarify → brief → generate queries → run queries → final report\n",
        "\n",
        "MAX_CLARIFICATION_ATTEMPTS = 3\n",
        "DEFAULT_NUM_QUERIES = 6\n",
        "\n",
        "\n",
        "def _history_to_text(history: History) -> History:\n",
        "    \"\"\"Ensure History is sanitized and ready.\"\"\"\n",
        "    return sanitize_history(history)\n",
        "\n",
        "\n",
        "def _parse_queries(text: str, limit: int) -> list:\n",
        "    lines = [q.strip(\"- \") for q in (text or \"\").splitlines() if q.strip()]\n",
        "    return lines[: int(limit)]\n",
        "\n",
        "\n",
        "def run_deep_research(user_message: str, *, allow_clarification: bool = True, num_queries: int = DEFAULT_NUM_QUERIES) -> dict:\n",
        "    \"\"\"Entry point returning a JSON-like dict with final_report and intermediates.\n",
        "\n",
        "    Inputs mirror LangGraph: user messages feed a multi-phase pipeline.\n",
        "    \"\"\"\n",
        "    if not isinstance(user_message, str) or not user_message.strip():\n",
        "        return {\"final_report\": \"\", \"status\": \"reject\", \"notes\": []}\n",
        "\n",
        "    # Update history\n",
        "    conversation_history.messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "    safe_history = _history_to_text(conversation_history)\n",
        "    date_str = get_today_str()\n",
        "\n",
        "    # Phase 1: Clarify (optional)\n",
        "    attempts = 0\n",
        "    if allow_clarification:\n",
        "        while attempts < MAX_CLARIFICATION_ATTEMPTS:\n",
        "            c = clarify_predict(messages=safe_history, date=date_str)\n",
        "            need = getattr(c, \"need_clarification\", False)\n",
        "            question = getattr(c, \"question\", \"\")\n",
        "            verification = getattr(c, \"verification\", \"\")\n",
        "            attempts += 1\n",
        "            if need and question:\n",
        "                # Ask user one clarifying question and stop here (mimic END)\n",
        "                conversation_history.messages.append({\"role\": \"assistant\", \"content\": question})\n",
        "                return {\"final_report\": \"\", \"status\": \"clarify\", \"question\": question}\n",
        "            else:\n",
        "                if verification:\n",
        "                    conversation_history.messages.append({\"role\": \"assistant\", \"content\": verification})\n",
        "                break\n",
        "\n",
        "    # Phase 2: Brief\n",
        "    b = brief_predict(messages=safe_history, date=date_str)\n",
        "    research_brief = getattr(b, \"research_brief\", \"\").strip()\n",
        "\n",
        "    # Phase 3: Generate queries\n",
        "    q_pred = queries_predict(messages=safe_history, research_brief=research_brief, date=date_str, num_queries=int(num_queries))\n",
        "    queries = _parse_queries(getattr(q_pred, \"queries_text\", \"\"), limit=int(num_queries))\n",
        "\n",
        "    # Phase 4: Run queries (OpenAI-only search)\n",
        "    findings = openai_search(queries=queries, max_results=len(queries) or int(num_queries), topic=\"general\")\n",
        "\n",
        "    # Phase 5: Final report\n",
        "    report = final_report_predict(research_brief=research_brief, messages=safe_history, findings=findings, date=date_str)\n",
        "    final_report = getattr(report, \"final_report\", \"\")\n",
        "\n",
        "    # Append to history\n",
        "    if final_report.strip():\n",
        "        conversation_history.messages.append({\"role\": \"assistant\", \"content\": final_report})\n",
        "\n",
        "    return {\n",
        "        \"final_report\": final_report,\n",
        "        \"status\": \"ok\",\n",
        "        \"research_brief\": research_brief,\n",
        "        \"queries\": queries,\n",
        "        \"findings\": findings,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examples / smoke tests\n",
        "\n",
        "print(\"\\n--- Clarify path (likely) ---\")\n",
        "resp = run_deep_research(\"I need a marketing plan\")\n",
        "print({k: resp[k] for k in resp if k in (\"status\", \"question\")})\n",
        "\n",
        "print(\"\\n--- Direct report path ---\")\n",
        "resp2 = run_deep_research(\"Compare Tesla FSD approaches vs Waymo safety stack in 2024.\")\n",
        "print({k: (resp2[k][:120] + \"...\") if isinstance(resp2.get(k), str) and k == \"final_report\" else resp2[k] for k in resp2 if k in (\"status\", \"research_brief\", \"queries\", \"final_report\")})\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
